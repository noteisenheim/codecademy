{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from preprocessing import *\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "batch_size = 64\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder training setup\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_hidden, state_cell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder training setup:\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 434)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 756)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 707584      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  1037312     input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 756)    194292      lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,939,188\n",
      "Trainable params: 1,939,188\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "print(\"Model summary:\\n\")\n",
    "training_model.summary()\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/noteisenheim/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/noteisenheim/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 800 samples, validate on 201 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 1.8595 - accuracy: 0.0719 - val_loss: 1.5691 - val_accuracy: 0.0769\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 1.4030 - accuracy: 0.1014 - val_loss: 1.5777 - val_accuracy: 0.0972\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 1.3182 - accuracy: 0.1112 - val_loss: 1.5979 - val_accuracy: 0.0796\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 1.2633 - accuracy: 0.1215 - val_loss: 1.5733 - val_accuracy: 0.0968\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 1.2210 - accuracy: 0.1292 - val_loss: 1.5863 - val_accuracy: 0.0922\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 1.1784 - accuracy: 0.1364 - val_loss: 1.6133 - val_accuracy: 0.0972\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 1.1366 - accuracy: 0.1406 - val_loss: 1.6457 - val_accuracy: 0.0987\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 1.0968 - accuracy: 0.1465 - val_loss: 1.6816 - val_accuracy: 0.0972\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 1.0694 - accuracy: 0.1509 - val_loss: 1.6013 - val_accuracy: 0.1152\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 1.0431 - accuracy: 0.1547 - val_loss: 1.6543 - val_accuracy: 0.1140\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 1.0088 - accuracy: 0.1589 - val_loss: 1.7120 - val_accuracy: 0.1102\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.9855 - accuracy: 0.1637 - val_loss: 1.6636 - val_accuracy: 0.1232\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.9548 - accuracy: 0.1695 - val_loss: 1.6656 - val_accuracy: 0.1297\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.9280 - accuracy: 0.1732 - val_loss: 1.6453 - val_accuracy: 0.1351\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.9052 - accuracy: 0.1771 - val_loss: 1.6663 - val_accuracy: 0.1286\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.8779 - accuracy: 0.1888 - val_loss: 1.6911 - val_accuracy: 0.1324\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.8509 - accuracy: 0.1891 - val_loss: 1.6630 - val_accuracy: 0.1427\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.8330 - accuracy: 0.1933 - val_loss: 1.6657 - val_accuracy: 0.1343\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.8063 - accuracy: 0.1947 - val_loss: 1.6592 - val_accuracy: 0.1393\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.7870 - accuracy: 0.1977 - val_loss: 1.6767 - val_accuracy: 0.1316\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.7640 - accuracy: 0.2139 - val_loss: 1.6430 - val_accuracy: 0.1439\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.7476 - accuracy: 0.2331 - val_loss: 1.6702 - val_accuracy: 0.1405\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.7196 - accuracy: 0.2143 - val_loss: 1.6512 - val_accuracy: 0.1420\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.7036 - accuracy: 0.2102 - val_loss: 1.6605 - val_accuracy: 0.1439\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.6846 - accuracy: 0.2447 - val_loss: 1.6669 - val_accuracy: 0.1439\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.6714 - accuracy: 0.2409 - val_loss: 1.6760 - val_accuracy: 0.1351\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.6498 - accuracy: 0.2297 - val_loss: 1.6370 - val_accuracy: 0.1588\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.6336 - accuracy: 0.2395 - val_loss: 1.6429 - val_accuracy: 0.1565\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.6189 - accuracy: 0.2448 - val_loss: 1.6646 - val_accuracy: 0.1508\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.6066 - accuracy: 0.2486 - val_loss: 1.6554 - val_accuracy: 0.1665\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.5853 - accuracy: 0.2970 - val_loss: 1.6795 - val_accuracy: 0.1703\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.5763 - accuracy: 0.2825 - val_loss: 1.6379 - val_accuracy: 0.1676\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.5591 - accuracy: 0.2859 - val_loss: 1.6564 - val_accuracy: 0.2518\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.5487 - accuracy: 0.3703 - val_loss: 1.6352 - val_accuracy: 0.2277\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.5325 - accuracy: 0.4129 - val_loss: 1.6370 - val_accuracy: 0.2197\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.5165 - accuracy: 0.4290 - val_loss: 1.6640 - val_accuracy: 0.3525\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.5073 - accuracy: 0.4248 - val_loss: 1.6558 - val_accuracy: 0.2212\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.4920 - accuracy: 0.4428 - val_loss: 1.6600 - val_accuracy: 0.3888\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4817 - accuracy: 0.4886 - val_loss: 1.6451 - val_accuracy: 0.2296\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4665 - accuracy: 0.5013 - val_loss: 1.6915 - val_accuracy: 0.4696\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.4549 - accuracy: 0.5287 - val_loss: 1.6437 - val_accuracy: 0.4129\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4447 - accuracy: 0.5315 - val_loss: 1.6620 - val_accuracy: 0.3865\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.4385 - accuracy: 0.5328 - val_loss: 1.6690 - val_accuracy: 0.4413\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4197 - accuracy: 0.5464 - val_loss: 1.7260 - val_accuracy: 0.4979\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4127 - accuracy: 0.5496 - val_loss: 1.7011 - val_accuracy: 0.4206\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.4024 - accuracy: 0.5450 - val_loss: 1.6769 - val_accuracy: 0.4546\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.3899 - accuracy: 0.5615 - val_loss: 1.6689 - val_accuracy: 0.5549\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.3841 - accuracy: 0.5687 - val_loss: 1.6900 - val_accuracy: 0.5281\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.3669 - accuracy: 0.6045 - val_loss: 1.6750 - val_accuracy: 0.4906\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.3632 - accuracy: 0.5912 - val_loss: 1.7107 - val_accuracy: 0.6326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe6b531afd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "training_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.save('training_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_inp = training_model.input[0]\n",
    "enc_out, state_h_enc, state_c_enc = training_model.layers[2].output\n",
    "enc_states = [state_h_enc, state_c_enc]\n",
    "\n",
    "enc_model = Model(enc_inp, enc_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_state_inp_h = Input(shape=(latent_dim, ))\n",
    "dec_state_inp_c = Input(shape=(latent_dim, ))\n",
    "dec_state_inp = [dec_state_inp_h, dec_state_inp_c]\n",
    "dec_out, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=dec_state_inp)\n",
    "dec_states = [state_h, state_c]\n",
    "dec_out = decoder_dense(dec_out)\n",
    "\n",
    "dec_model = Model([decoder_inputs] + dec_state_inp, [dec_out] + dec_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(test_input):\n",
    "    # encoding input as state vectors\n",
    "    states_v = enc_model.predict(test_input)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
    "    \n",
    "    decoded_sent = ''\n",
    "    \n",
    "    stop = False\n",
    "    while not stop:\n",
    "        out_tokens, h_state, c_state = dec_model.predict([target_seq] + states_v)\n",
    "        s_token_idx = np.argmax(out_tokens[0, -1, :])\n",
    "        s_token = reverse_target_features_dict[s_token_idx]\n",
    "        decoded_sent += ' ' + s_token\n",
    "        \n",
    "        if s_token == '<END>' or len(decoded_sent) > max_decoder_seq_length:\n",
    "            stop = True\n",
    "            \n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, s_token_idx] = 1.\n",
    "        \n",
    "        states_v = [h_state, c_state]\n",
    "    return decoded_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence:  Komm raus . <END>\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence:  Warte ! <END>\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence:  Warte ! <END>\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence:  ! <END>\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence:  Lauf ! <END>\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence:  Fantastisch !\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence:  Fantastisch !\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence:  Feuer ! <END>\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence:  Nicht ! <END>\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence:  Nicht ! <END>\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence:  Nicht ! <END>\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence:  Warte ! <END>\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence:  Warte nicht !\n",
      "-\n",
      "Input sentence: Begin.\n",
      "Decoded sentence:  Ruf mich an .\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence:  Mach dich ! <END>\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence:  Warte ! <END>\n",
      "-\n",
      "Input sentence: Hurry!\n",
      "Decoded sentence:  Beeil ! <END>\n",
      "-\n",
      "Input sentence: Hurry!\n",
      "Decoded sentence:  Beeil ! <END>\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence:  Ich werde . <END>\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence:  Ich verstehe .\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(20):\n",
    "  test_input = encoder_input_data[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_seq(test_input)\n",
    "  print('-')\n",
    "  print('Input sentence:', input_docs[seq_index])\n",
    "  print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
